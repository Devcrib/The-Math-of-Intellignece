"""  @author Victor I. Afolabi  A.I. Engineer & Software developer  javafolabi@gmail.com  Created on 25 August, 2017 @ 8:15 PM.  Copyright (c) 2017. victor. All rights reserved."""# Linear Regression with gradient descentimport numpy as npdef squared_error(data, m, b):    total_error = 0    for i, d in enumerate(data):        x = d[0]        y = d[1]        total_error += (y - (m * x + b)) ** 2    return float(total_error) / len(data)def train():    return [0, 0]def start():    x = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], [1, 0, 1]])    y = np.array([[0], [0], [0], [1], [0], [1]])    data = x, y    m, b = [0, 0]    err = squared_error(data, m, b)    print('Error for m = {:.2f} and b = {:.2f} is {:.4f}'.format(m, b, err))if __name__ == '__main__':    start()